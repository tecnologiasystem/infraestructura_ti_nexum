import pandas as pd
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from datetime import datetime, timedelta, date
from app.dal.contactabilidad_dal import engine

"""
Este m√≥dulo contiene la l√≥gica principal para:
- Cargar los datos necesarios desde SQL Server.
- Entrenar un modelo predictivo usando XGBoost para estimar la contactabilidad.
- Generar predicciones futuras para cada n√∫mero telef√≥nico, por d√≠a y hora.
- Evaluar el modelo mediante m√©tricas cl√°sicas (accuracy, precision, recall, F1, ROC AUC).

Librer√≠as utilizadas:
- pandas: para manipulaci√≥n de datos.
- xgboost: modelo de clasificaci√≥n optimizado para alto rendimiento.
- sklearn: para codificaci√≥n, m√©tricas y divisi√≥n de datos.
- datetime: para trabajar con fechas y generar fechas futuras.
- contactabilidad_dal.engine: conexi√≥n SQLAlchemy hacia la base de datos.
"""


def calcular_proxima_fecha(dia_objetivo):
    """
    Calcula la pr√≥xima fecha (en formato YYYY-MM-DD) en que ocurrir√° un d√≠a espec√≠fico de la semana.

    Par√°metros:
    - dia_objetivo (str): D√≠a objetivo en ingl√©s (por ejemplo, 'Monday', 'Tuesday', etc.)

    L√≥gica:
    - Se obtiene la fecha actual con date.today().
    - Se transforma el nombre del d√≠a objetivo a su √≠ndice correspondiente (0 = Monday, 6 = Sunday).
    - Se compara con el d√≠a actual de la semana para saber cu√°ntos d√≠as faltan.
    - Si el d√≠a objetivo es hoy, se asume que se quiere la pr√≥xima ocurrencia (por eso suma 7).
    - Retorna la fecha futura formateada como string.

    Ejemplo:
    Si hoy es martes (√≠ndice 1) y el objetivo es 'Friday' (√≠ndice 4),
    entonces faltan 3 d√≠as y la funci√≥n retornar√° la fecha del pr√≥ximo viernes.
    """
    hoy = date.today()
    dias = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    objetivo_idx = dias.index(dia_objetivo)
    hoy_idx = hoy.weekday()
    dias_adelante = (objetivo_idx - hoy_idx + 7) % 7
    if dias_adelante == 0:
        dias_adelante = 7
    return (hoy + timedelta(days=dias_adelante)).strftime('%Y-%m-%d')


def cargar_datos(limit=5000):
    """
    Carga y preprocesa los datos necesarios para el modelo de predicci√≥n de contactabilidad.

    Par√°metros:
    - limit (int): N√∫mero m√°ximo de registros que se desean consultar desde la base de datos.

    Proceso:
    1. Se realiza la conexi√≥n con la base de datos a trav√©s del motor `engine`.
    2. Se consulta la tabla 'unificado' que contiene el historial de llamadas.
    3. Se consulta la tabla 'pesofin' que contiene las frases categorizadas con pesos (0 o 1),
       donde 1 representa un contacto exitoso y 0 un intento fallido.
    4. Se realiza una fusi√≥n (merge) entre los datos de llamadas y los pesos, 
       uniendo las filas por coincidencia entre `status_name` del historial y `frase` de la tabla de pesos.
    5. Si alguna de estas operaciones falla, se captura la excepci√≥n y retorna un DataFrame vac√≠o.

    Retorna:
    - Un DataFrame con los datos de llamadas enriquecidos con la columna de `peso` correspondiente
      a cada `status_name`.
    """
    print("üîÑ Cargando datos desde SQL...")
    try:
        df_llamadas = pd.read_sql(f"SELECT TOP({limit}) * FROM unificado", engine)
        print("‚úÖ Datos de llamadas cargados")
        df_pesos = pd.read_sql("SELECT * FROM pesofin", engine)
        print("‚úÖ Datos de pesos cargados")
    except Exception as e:
        print("‚ùå Error al conectar con la base de datos:", e)
        return pd.DataFrame()

    print("üîó Uniendo datos...")
    merged = df_llamadas.merge(
        df_pesos[['frase', 'peso']],
        how='left',
        left_on='status_name',
        right_on='frase'
    )


    """
    6. Se filtran √∫nicamente los registros que tienen un valor de peso v√°lido (0 o 1), 
       ignorando los que no tengan esta clasificaci√≥n.
    """
    df = merged[merged['peso'].isin([0, 1])].copy()

    """
    7. Se convierte la columna 'date_call' al tipo datetime para asegurar un correcto an√°lisis temporal.
       - `dayfirst=True` asegura que el formato d√≠a/mes/a√±o sea interpretado correctamente.
       - `errors='coerce'` convierte fechas inv√°lidas en NaT (valores nulos de fecha).
    """
    df['date_call'] = pd.to_datetime(df['date_call'], dayfirst=True, errors='coerce')

    """
    8. Se eliminan las filas donde no se pudo convertir la fecha (es decir, donde 'date_call' es nulo).
    """
    df = df.dropna(subset=['date_call'])

    """
    9. Se extrae el nombre del d√≠a de la semana (ej. 'Monday') y la hora (0-23) de la fecha de la llamada,
       y se agregan como nuevas columnas para alimentar el modelo.
    """
    df['dia'] = df['date_call'].dt.day_name()
    df['hora'] = df['date_call'].dt.hour

    """
    10. Se imprime la cantidad de registros finales que ser√°n utilizados para el entrenamiento.
    """
    print(f"üìä Datos listos: {len(df)} registros v√°lidos")

    """
    11. Se devuelve un DataFrame con las columnas relevantes:
        - 'telephone': n√∫mero de tel√©fono
        - 'dia': d√≠a de la semana
        - 'hora': hora del d√≠a
        - 'peso': si respondi√≥ (1) o no respondi√≥ (0)
    """
    return df[['telephone', 'dia', 'hora', 'peso']]


# üß† Entrenamiento del modelo con XGBoost
def entrenar_modelo(df):
    """
    Esta funci√≥n entrena un modelo XGBoost para predecir la probabilidad de contacto exitoso.
    """

    print("ü§ñ Entrenando modelo con XGBoost...")

    """
    1. Se instancian dos codificadores LabelEncoder:
       - le_dia para convertir el nombre del d√≠a (ej: 'Monday') a un n√∫mero entero.
       - le_tel para codificar cada n√∫mero de tel√©fono con un valor num√©rico.
       Esto es necesario porque XGBoost no trabaja directamente con variables categ√≥ricas.
    """
    le_dia = LabelEncoder()
    le_tel = LabelEncoder()

    """
    2. Se crea una copia del DataFrame con las columnas relevantes para el modelo.
       Luego se agregan las columnas codificadas 'telefono_encoded' y 'dia_encoded'.
    """
    X = df[['telephone', 'dia', 'hora']].copy()
    X['telefono_encoded'] = le_tel.fit_transform(X['telephone'])
    X['dia_encoded'] = le_dia.fit_transform(X['dia'])

    """
    3. Se seleccionan las variables que se usar√°n como caracter√≠sticas (X) del modelo:
       - telefono_encoded: ID √∫nico para cada n√∫mero.
       - dia_encoded: d√≠a de la semana en forma num√©rica.
       - hora: hora del d√≠a (0-23).
       La variable objetivo (y) ser√° 'peso', que indica si contest√≥ (1) o no (0).
    """
    X_model = X[['telefono_encoded', 'dia_encoded', 'hora']]
    y = df['peso']

    """
    4. Se configura y entrena el modelo XGBoostClassifier con par√°metros ajustados para priorizar la precisi√≥n:
       - n_estimators=350: cantidad de √°rboles.
       - max_depth=7: profundidad m√°xima de cada √°rbol.
       - learning_rate=0.05: velocidad de aprendizaje (menor es m√°s preciso, pero m√°s lento).
       - scale_pos_weight=5: se da m√°s peso a las clases positivas (1 = contest√≥).
       - subsample y colsample_bytree: reducci√≥n para evitar overfitting.
       - use_label_encoder=False y eval_metric='aucpr': m√©tricas m√°s precisas para desequilibrio.
    """
    model = XGBClassifier(
        n_estimators=350,
        max_depth=7,
        learning_rate=0.05,
        scale_pos_weight=5,
        subsample=0.9,
        colsample_bytree=0.8,
        use_label_encoder=False,
        eval_metric='aucpr',
        random_state=42
    )

    """
    5. Se define un vector de pesos para las muestras:
       - Las clases positivas (respuesta = 1) reciben un peso mayor para que el modelo las tome m√°s en serio.
    """
    sample_weights = y.map(lambda val: 5 if val == 1 else 1)

    """
    6. Se entrena el modelo usando los datos y los pesos.
    """
    model.fit(X_model, y, sample_weight=sample_weights)

    print("‚úÖ Modelo entrenado con XGBoost")

    """
    7. Se devuelven el modelo y los codificadores para poder usarlos despu√©s en predicciones.
    """
    return model, le_tel, le_dia


# üîÆ Predicci√≥n por contacto y horario
def generar_predicciones(model, le_tel, le_dia, df):
    """
    Esta funci√≥n genera predicciones de contacto exitoso por cada n√∫mero telef√≥nico,
    para cada d√≠a de la semana y cada hora del d√≠a. Se retorna solo la mejor predicci√≥n
    para cada tel√©fono (aquella con la probabilidad m√°s alta que supere un umbral).
    """

    print("üîÆ Generando predicciones futuras por n√∫mero...")

    """
    1. Se extraen los tel√©fonos √∫nicos del DataFrame de entrada.
       Tambi√©n se obtienen los d√≠as de la semana codificados y las 24 horas del d√≠a.
    """
    contactos = df['telephone'].unique()
    dias_semana = list(le_dia.classes_)
    horas = list(range(0, 24))

    """
    2. Se inicializa la lista de predicciones.
    """
    predicciones = []

    """
    3. Se realiza una predicci√≥n para cada combinaci√≥n de:
       - Tel√©fono
       - D√≠a de la semana
       - Hora (0 a 23)

       Por cada combinaci√≥n, se calcula la probabilidad de que ese tel√©fono conteste
       en ese d√≠a y hora. Si la probabilidad supera el 98%, se guarda la predicci√≥n.
    """
    for tel in contactos:
        # Codificamos el n√∫mero de tel√©fono
        tel_encoded = le_tel.transform([tel])[0]

        for dia in dias_semana:
            # Codificamos el d√≠a de la semana
            dia_encoded = le_dia.transform([dia])[0]

            for hora in horas:
                # Se construye el DataFrame de una sola fila para predecir
                X_pred = pd.DataFrame(
                    [[tel_encoded, dia_encoded, hora]],
                    columns=['telefono_encoded', 'dia_encoded', 'hora']
                )

                # Se obtiene la probabilidad de clase positiva (√≠ndice 1)
                prob = model.predict_proba(X_pred)[0][1] * 100

                # Se filtran solo las predicciones con alta probabilidad (‚â• 98%)
                if prob >= 98:
                    predicciones.append({
                        'telefono': tel,
                        'dia': dia,
                        'hora': hora,
                        'probabilidad_contacto': round(prob, 1)
                    })


        """
    4. Se convierten las predicciones v√°lidas (probabilidad ‚â• 98%) a un DataFrame.
       Cada fila representa una combinaci√≥n de tel√©fono, d√≠a y hora con alta probabilidad.
    """
    df_pred = pd.DataFrame(predicciones)

    print("‚úÖ Predicciones generadas")

    """
    5. Se ordenan las predicciones por:
        - Tel√©fono (ascendente)
        - Probabilidad de contacto (descendente)
    
       Esto permite que la mejor predicci√≥n por cada tel√©fono quede en la primera posici√≥n.
    """
    mejores = df_pred.sort_values(['telefono', 'probabilidad_contacto'], ascending=[True, False])

    """
    6. Se agrupa el DataFrame por 'telefono' y se toma solo la primera fila de cada grupo,
       que corresponde a la mejor predicci√≥n para ese n√∫mero.
    """
    mejores_final = mejores.groupby('telefono').first().reset_index()

    """
    7. A cada predicci√≥n se le agrega una columna con la 'fecha futura recomendada',
       la cual se calcula usando la funci√≥n `calcular_proxima_fecha` seg√∫n el d√≠a con mejor probabilidad.
    """
    mejores_final['fecha_futura_recomendada'] = mejores_final['dia'].apply(calcular_proxima_fecha)

    """
    8. Finalmente, se retorna el DataFrame que contiene:
        - El tel√©fono
        - El mejor d√≠a y hora para contactarlo
        - La probabilidad estimada
        - La fecha futura recomendada para intentarlo
    """
    return mejores_final


# üìä Evaluaci√≥n del modelo con umbral fijo alto (prioriza precisi√≥n)
def obtener_metricas():
    """
    Esta funci√≥n entrena un modelo con una partici√≥n de los datos hist√≥ricos
    y eval√∫a su rendimiento utilizando varias m√©tricas (accuracy, precision, recall, etc).
    Se emplea un umbral de predicci√≥n alto para priorizar la precisi√≥n del modelo.
    """
    print("üìÖ Obteniendo m√©tricas del modelo...")

    """
    1. Cargar los datos de llamadas con un l√≠mite de 5000 registros.
       Si ocurre alg√∫n error y el DataFrame resulta vac√≠o, se retorna un mensaje de error.
    """
    df = cargar_datos(limit=5000)
    if df.empty:
        return {
            'error': 'No se pudo cargar la data para evaluar el modelo'
        }

    """
    2. Preparar codificadores LabelEncoder para convertir variables categ√≥ricas:
       - `dia` (nombre del d√≠a de la semana)
       - `telephone` (n√∫mero de tel√©fono)
    """
    le_dia = LabelEncoder()
    le_tel = LabelEncoder()

    """
    3. Crear un nuevo DataFrame de entrada `X` con las columnas relevantes,
       luego se agregan las columnas codificadas para los algoritmos de ML.
    """
    X = df[['telephone', 'dia', 'hora']].copy()
    X['telefono_encoded'] = le_tel.fit_transform(X['telephone'])
    X['dia_encoded'] = le_dia.fit_transform(X['dia'])

    """
    4. Seleccionar las columnas finales para el modelo (`X_model`) y la variable objetivo (`y`).
    """
    X_model = X[['telefono_encoded', 'dia_encoded', 'hora']]
    y = df['peso']  # Etiqueta: 1 = contest√≥, 0 = no contest√≥

    """
    5. Dividir los datos en conjunto de entrenamiento (80%) y prueba (20%).
    """
    X_train, X_test, y_train, y_test = train_test_split(X_model, y, test_size=0.2, random_state=42)

    """
    6. Definir el modelo XGBoost con hiperpar√°metros ajustados para manejar desbalanceo.
    """
    model = XGBClassifier(
        n_estimators=350,
        max_depth=7,
        learning_rate=0.05,
        scale_pos_weight=5,
        subsample=0.9,
        colsample_bytree=0.8,
        use_label_encoder=False,
        eval_metric='aucpr',
        random_state=42
    )

    """
    7. Crear pesos personalizados para las muestras de entrenamiento:
       Las respuestas positivas (peso=1) se ponderan m√°s para mejorar la precisi√≥n.
    """
    sample_weights = y_train.map(lambda val: 5 if val == 1 else 1)

    """
    8. Entrenar el modelo usando los pesos definidos.
    """
    model.fit(X_train, y_train, sample_weight=sample_weights)

    """
    9. Obtener las probabilidades predichas para el conjunto de prueba.
       Luego aplicar un umbral de 0.98 para considerar una predicci√≥n como positiva.
    """
    y_prob = model.predict_proba(X_test)[:, 1]
    threshold = 0.98  # Umbral alto para priorizar precisi√≥n
    y_pred = (y_prob >= threshold).astype(int)

    print("‚úÖ M√©tricas calculadas con umbral fijo (alta precisi√≥n)")

    """
    10. Calcular y retornar las m√©tricas principales:
        - accuracy: proporci√≥n de predicciones correctas
        - precision: de los que predijo positivos, cu√°ntos realmente lo fueron
        - recall: de todos los positivos reales, cu√°ntos detect√≥
        - f1: balance entre precisi√≥n y recall
        - roc_auc: √°rea bajo curva ROC
    """
    return {
        'threshold_usado': threshold,
        'accuracy': round(float(accuracy_score(y_test, y_pred)), 2),
        'precision': round(float(precision_score(y_test, y_pred)), 2),
        'recall': round(float(recall_score(y_test, y_pred)), 2),
        'f1': round(float(f1_score(y_test, y_pred)), 2),
        'roc_auc': round(float(roc_auc_score(y_test, y_prob)), 2)
    }
